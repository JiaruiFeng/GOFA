root_path: "./"
data_root_path: "../TAGDataset"
llama_pretrain_checkpoint: "./cache_data/model/llama-2-7b-chat-finetuned-icae_zeroweight_llama2.pt"
mistral_pretrain_checkpoint: "./cache_data/model/mistral_7b_ft_icae.safetensors"
num_bases: 4
emb_dim: 1024
num_layers: 6
mlp_type: "gp"
dropout: 0.0
JK: "last"
lr: 0.0003
l2: 0.1
grad_clip: 0.5
num_epochs: 1
last_epochs: 0
batch_size: 1
eval_batch_size: 1
num_workers: 4
seed: 1
data_path:
offline_log: False
test_rep: 1
log_project: "GOFA"
exp_name: "gofa"
train_sample_size: -1
eval_sample_size: -1
rwpe:
task_names:
  - arxiv
llm_name: "icae"
base_llm: "mistral7b"
llm_max_length: 128
llm_b_size: 1
val_interval: 1
load_texts: True
mode: "autoencoder"
temp: 0.1
compressed_layer: 32
max_nodes_per_hop: 5
grad_acc_step: 8
save_model:
  save: False
  steps:
  monitor: False
  time: 5
  epochs:
  top_k: -1
  last: True
load_dir: "./best_ckpt.pth"
load_model: False
last_save: True
training_precision: "bf16-mixed"
ckpt_path:
run_mode: "pretrain"
dec_lora: False
node_text: False